{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting words occurrences\n",
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:07:41\n",
      "Map reviews to integers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'album', 'song', 'the', 'quot']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:02\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pyprind\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "\n",
    "# Read csv file\n",
    "from rnn import SentimentRNN\n",
    "\n",
    "df = pd.read_csv('music_review.csv', encoding='utf-8')\n",
    "\n",
    "counts = Counter()\n",
    "pbar = pyprind.ProgBar(len(df['review']), title='Counting words occurrences')\n",
    "\n",
    "for i, review in enumerate(df['review']):\n",
    "    text = ''.join([c if c not in punctuation else ' '+c+' ' for c in str(review)]).lower()\n",
    "    df.loc[i, 'review'] = text\n",
    "    pbar.update()\n",
    "    counts.update(text.split())\n",
    "\n",
    "# Mapping the each unique word into an integer\n",
    "word_counts = sorted(counts, key=counts.get, reverse=True)\n",
    "print(word_counts[:5])\n",
    "word_to_int = {\n",
    "    word: ii for ii, word in enumerate(word_counts, 1)\n",
    "}\n",
    "\n",
    "mapped_reviews = []\n",
    "pbar = pyprind.ProgBar(len(df['review']), title='Map reviews to integers')\n",
    "for review in df['review']:\n",
    "    mapped_reviews.append([word_to_int[word] for word in review.split()])\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero padding process\n",
    "sequence_length = 200\n",
    "sequences = np.zeros((len(mapped_reviews), sequence_length), dtype=int)\n",
    "\n",
    "for i, row in enumerate(mapped_reviews):\n",
    "    review_arr = np.array(row)\n",
    "    sequences[i, -len(row):] = review_arr[-sequence_length:]\n",
    "\n",
    "# split data set into training part and testing part\n",
    "X_train = sequences[:44705, :]\n",
    "y_train = df.loc[:44705, 'sentiment'].values\n",
    "\n",
    "X_test = sequences[44705:, :]\n",
    "y_test = df.loc[44705:, 'sentiment'].values\n",
    "\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " << initial state >>  (LSTMStateTuple(c=<tf.Tensor 'MultiRNNCellZeroState/DropoutWrapperZeroState/BasicLSTMCellZeroState/zeros:0' shape=(100, 128) dtype=float32>, h=<tf.Tensor 'MultiRNNCellZeroState/DropoutWrapperZeroState/BasicLSTMCellZeroState/zeros_1:0' shape=(100, 128) dtype=float32>),)\n",
      "\n",
      " << lstm output >> Tensor(\"rnn/transpose:0\", shape=(100, 200, 128), dtype=float32)\n",
      "\n",
      " << final state >> (LSTMStateTuple(c=<tf.Tensor 'rnn/while/Exit_2:0' shape=(100, 128) dtype=float32>, h=<tf.Tensor 'rnn/while/Exit_3:0' shape=(100, 128) dtype=float32>),)\n",
      "\n",
      " << logits >> Tensor(\"logits_squeezed:0\", shape=(100,), dtype=float32)\n",
      "\n",
      " << predictions >>  {'probabilities': <tf.Tensor 'probabilities:0' shape=(100,) dtype=float32>, 'labels': <tf.Tensor 'labels:0' shape=(100,) dtype=int32>}\n",
      "Epoch: 1/40 Iteration: 20 | Train loss: 0.54186\n",
      "Epoch: 1/40 Iteration: 40 | Train loss: 0.24232\n",
      "Epoch: 1/40 Iteration: 60 | Train loss: 0.43569\n",
      "Epoch: 1/40 Iteration: 80 | Train loss: 0.38813\n",
      "Epoch: 1/40 Iteration: 100 | Train loss: 0.36161\n",
      "Epoch: 1/40 Iteration: 120 | Train loss: 0.33501\n",
      "Epoch: 1/40 Iteration: 140 | Train loss: 0.31031\n",
      "Epoch: 1/40 Iteration: 160 | Train loss: 0.15878\n"
     ]
    }
   ],
   "source": [
    "n_words = max(list(word_to_int.values())) + 1\n",
    "\n",
    "rnn = SentimentRNN(n_words=n_words,\n",
    "                   seq_len=sequence_length,\n",
    "                   embed_size=256,\n",
    "                   lstm_size=128,\n",
    "                   num_layers=1,\n",
    "                   batch_size=100,\n",
    "                   learning_rate=0.001)\n",
    "\n",
    "rnn.train(X_train, y_train, num_epochs=40)\n",
    "preds = rnn.predict(X_test)\n",
    "y_true = y_test[:len(preds)]\n",
    "print('Test acc.: %.3f' % (np.sum(preds==y_true)/len(y_true)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
